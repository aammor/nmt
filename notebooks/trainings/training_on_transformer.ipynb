{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c39e4ff-d158-4b2f-801d-687247726209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch,inspect\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "from dev import namespace_tools\n",
    "# nested namespace arguement containing all elements associated to the training setup\n",
    "\n",
    "notebook_run = Namespace(\n",
    "    simple_hp = Namespace(\n",
    "        batch_size= 32,\n",
    "        d_model = 64,\n",
    "        early_stop_thresh = np.inf, # default to np.inf\n",
    "        nb_epochs = 500,\n",
    "        warm_up_epochs = 20,\n",
    "    ),\n",
    "    # parameters to limit the size of the dataset\n",
    "    dset_truncation = Namespace(\n",
    "        limit_length= 1,\n",
    "        use_splitting = False,\n",
    "        max_length_from_file = False,\n",
    "    ),\n",
    "    # parameters for the optimization algorithm\n",
    "    opt_params = Namespace(\n",
    "        optimizer = partial(torch.optim.NAdam,lr=0.001),\n",
    "        scheduler = partial(torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,15,T_mult=2,eta_min=10**(-6))\n",
    "    ),\n",
    "    # parameters to reload the model\n",
    "    train_state_control = Namespace(             \n",
    "        load_from_backup = False,\n",
    "        restore_optimizer = False\n",
    "    ),\n",
    "    #paths from root\n",
    "    paths = Namespace(\n",
    "        path_dataset = \"data/french_english_dataset/fra.txt\",\n",
    "        path_language_info = \"models/language_info.pth\",\n",
    "        path_dataset_splitting = \"dataset_splitting\",\n",
    "        path_model_and_dependencies = \"models/sequence_translator_transformer_new.pth\"\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "notebook_run = namespace_tools.NameSpaceAggregation(notebook_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8c7025-ed36-4f5f-9ee8-68dd2100dfd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_class = torch.optim.NAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86eb3f18-3a22-40f9-b6e6-f1c51099248e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dev import module_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a055d-f0e8-4f60-8ad9-8c31f6654bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialization = module_io.serialize(request_class)\n",
    "class_name,module_name = serialization\n",
    "loaded_class = module_io.get_callable(class_name,module_name)\n",
    "loaded_class == request_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f5b541d-06c7-4434-b6bb-4851d4bdaa45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.optim.nadam' from '/root/miniconda/lib/python3.9/site-packages/torch/optim/nadam.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "module = importlib.import_module('torch.optim.nadam')\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "375d2e19-898c-41cf-a1b6-9187846600e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "the_class = torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "serialization = module_io.serialize(the_class)\n",
    "loaded_class = module_io.get_module_from_name(*serialization)\n",
    "loaded_class,serialization\n",
    "assert loaded_class == the_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617651d-20e5-471d-b8b4-728273f5816e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_module(request_class.__name__,get_name_module(request_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63eb27-ab5f-4108-9b1a-ecd6b34888df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notebook_run_new == notebook_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e86856-37c6-45b7-b5a5-ba7caa814744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set(state_dict[\"opt_params\"][\"optimizer\"].keys()) == {'func','args','keywords'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c67373-dd68-4524-b336-c20e333f12de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ploomber_engine.ipython import PloomberClient\n",
    "from ploomber import DAG\n",
    "from pathlib import Path\n",
    "from ploomber.products import File\n",
    "\n",
    "# initialize client\n",
    "client = PloomberClient.from_path(Path(\"./training_setup.ipynb\"))#,cwd=Path(\"../../\"))\n",
    "from argparse import Namespace\n",
    "\n",
    "from translation_machine.models import transformer_mod\n",
    "from translation_machine import sentence_mod\n",
    "\n",
    "initial_namespace_as_dict = {key:globals()[key] for key in [\"simple_hyp_params\",\"dset_truncation\",\n",
    "                                                                         \"optimizer_creator\",\"scheduler_creator\",\n",
    "                                                                         \"train_state_control\",\"paths\"]}\n",
    "train_setup = client.get_namespace(initial_namespace_as_dict)\n",
    "for key,val in train_setup.items():\n",
    "        globals()[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa554096-29cc-4c90-b8aa-0530d6f01fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(PloomberClient.from_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d6670-c590-4d63-af90-4a57d1a75b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# revert to train mode\n",
    "model.train()\n",
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa421e9-c523-4f00-a33a-e5a3e7ee6f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from translation_machine import model_trainer_mod\n",
    "\n",
    "model_trainer = model_trainer_mod.ModelTrainer(model,optimizer,scheduler,train_data_loader,val_data_loader,baseline_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0e929-d502-4190-ae9f-18a30bf26ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import argparse\n",
    "\n",
    "@dataclass\n",
    "class NotebookRun:\n",
    "    simple_hyp_params : argparse.Namespace\n",
    "    optimization_control : argparse.Namespace\n",
    "    dataset_control : argparse.Namespace\n",
    "    state_train_control : argparse.Namespace\n",
    "    paths_from_training : argparse.Namespace\n",
    "    \n",
    "    def __hash__(self):\n",
    "\n",
    "        hash_value = hash(self.__to_dict__)\n",
    "        return hash_value\n",
    "    \n",
    "    def state_dict(self):\n",
    "        as_dict = {key:self.__dict__[key] for key in [\"simple_hyp_params\",\"optimization_control\",            \n",
    "            \"dataset_control\",\"state_train_control\",            \n",
    "            \"paths_from_training\"]\n",
    "                  }\n",
    "        return as_dict\n",
    "    def load_state_dict(self,state_dict):\n",
    "        self.__dict__.update(**state_dict)\n",
    "    \n",
    "    def __eq__(self,other):\n",
    "        for el in [\"simple_hyp_params\",\"optimization_control\",            \n",
    "            \"dataset_control\",\"state_train_control\",            \n",
    "            \"paths_from_training\"]:\n",
    "            if self.__dict__[el] != other.__dict__[el]:\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "opt_params = argparse.Namespace(optimizer=optimizer,\n",
    "                               scheduler=scheduler)\n",
    "\n",
    "notebook_run = NotebookRun(simple_hyp_params,opt_params,\n",
    "                           dset_truncation,train_state_control,\n",
    "                           paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81186763-87ae-4871-9dbc-9271a83c9f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## import matplotlib.pyplot as plt,numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "best_loss_val_mean = np.inf\n",
    "best_epoch = scheduler.last_epoch\n",
    "\n",
    "for epoch in tqdm(range(simple_hyp_params.nb_epochs)):\n",
    "    #import time\n",
    "    #start = time.time()\n",
    "    print(f\"training for epoch {epoch}\")\n",
    "    print(f\"for epoch {epoch} learning rate is {optimizer.param_groups[0]['lr']}\" )\n",
    "    print(\"training_step\")\n",
    "    loss_train,nb_words_per_batch_train,metric_train = model_trainer.train_on_epoch()\n",
    "    print(\"validation_step\")\n",
    "    loss_val,nb_words_per_batch_val,metric_val = model_trainer.validate_on_epoch()\n",
    "    \n",
    "    sum_loss_train = torch.tensor(loss_train).sum()\n",
    "    sum_loss_val = torch.tensor(loss_val).sum()\n",
    "    mean_train_loss = sum_loss_train/sum(nb_words_per_batch_train)\n",
    "    mean_val_loss = sum_loss_val/sum(nb_words_per_batch_val)\n",
    "    \n",
    "    print(f\"for epoch {epoch} mean loss on train {mean_train_loss}\")\n",
    "    print(f\"for epoch {epoch} mean loss on val {mean_val_loss}\")\n",
    "        \n",
    "    losses[\"train\"].append(mean_train_loss)\n",
    "    losses[\"val\"].append(mean_val_loss)\n",
    "    metrics[\"train\"].append(metric_train)\n",
    "    metrics[\"val\"].append(metric_val)\n",
    "    \n",
    "    if (mean_val_loss < best_loss_val_mean):\n",
    "        best_epoch = model_trainer.scheduler.last_epoch\n",
    "        best_loss_val_mean = mean_val_loss\n",
    "\n",
    "        model_training_state = {\"model_params\":model_trainer.model.state_dict(),\n",
    "                               \"model_inputs\":model_inputs,\n",
    "                              \"optimizer\":optimizer.state_dict(),\n",
    "                              \"scheduler\":scheduler.state_dict(),\n",
    "                              }\n",
    "        results = { \"losses\":losses,\n",
    "                   \"metrics\":metrics}\n",
    "        new_back_up = dict()\n",
    "        if \"back_up\" in globals():\n",
    "            new_back_up[\"notebook_runs\"] = back_up[\"notebook_runs\"] + tuple([notebook_run.state_dict()])\n",
    "        else:\n",
    "            new_back_up[\"notebook_runs\"] = tuple([notebook_run.state_dict()])\n",
    "\n",
    "        new_back_up[\"results\"] = results\n",
    "        new_back_up[\"model_training_state\"] = model_training_state\n",
    "        \n",
    "        back_up = new_back_up\n",
    "        torch.save(back_up,path_model_and_dependencies)\n",
    "        print(f\"saving for epoch {epoch}\")\n",
    "        \n",
    "        plt.plot(losses[\"train\"],\"b*\")\n",
    "        plt.plot(losses[\"val\"],\"g*\")\n",
    "        plt.title(\"losses\")\n",
    "        plt.savefig(\"loss_curve\")\n",
    "        #import pdb;pdb.set_trace()\n",
    "    elif epoch - best_epoch > simple_hyp_params.early_stop_thresh  and epoch > simple_hyp_params.warm_up_epochs:\n",
    "        print(\"Early stopped training at epoch %d\" % epoch)\n",
    "        break  # terminate the training loop\n",
    "\n",
    "    del loss_train,nb_words_per_batch_train,metric_train\n",
    "\n",
    "    del loss_val,nb_words_per_batch_val,metric_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571b27e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses[\"train\"],\"b*\")\n",
    "plt.plot(losses[\"val\"],\"g*\")\n",
    "plt.title(\"losses\")\n",
    "plt.savefig(f'test.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb1cb3-1e16-433c-be65-ebdffce502c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
