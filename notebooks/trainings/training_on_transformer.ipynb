{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bd57d6-eadf-470a-82fb-c7a43342f736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch,inspect\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "from dev import namespace_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c39e4ff-d158-4b2f-801d-687247726209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nested namespace arguement containing all elements associated to the training setup\n",
    "\n",
    "notebook_run = Namespace(\n",
    "    simple_hp = Namespace(\n",
    "        batch_size= 16,\n",
    "        d_model = 64,\n",
    "        early_stop_thresh = np.inf,\n",
    "        nb_epochs = 200,\n",
    "        warm_up_epochs = 20,\n",
    "    ),\n",
    "    # parameters to limit the size of the dataset\n",
    "    dset_truncation = Namespace(\n",
    "        limit_length= 64,\n",
    "        use_splitting = False,\n #set to False,if you want to overfit the model on the training set \n", 
    "        max_length_from_file = False,\n",
    "        recompute_vocabulary = True,\n",
    "    ),\n",
    "    # parameters for the optimization algorithm\n",
    "    opt_params = Namespace(\n",
    "        unlinked_optimizer = partial(torch.optim.NAdam,lr=0.0001),\n",
    "        unlinked_scheduler = partial(torch.optim.lr_scheduler.ReduceLROnPlateau, mode='min', \n",
    "                                     factor=0.8, patience=5,min_lr=10**(-6))\n",
    "    ),\n",
    "    # parameters to reload the model\n",
    "    train_state_control = Namespace(             \n",
    "        load_from_backup =True,\n",
    "        restore_optimizer = True\n",
    "    ),\n",
    "    #paths from root\n",
    "    paths = namespace_tools.Paths(\n",
    "        path_dataset = \"data/french_english_dataset/fra.txt\",\n",
    "        path_language_info = \"models/language_info.pth\",\n",
    "        path_dataset_splitting = \"dataset_splitting\",\n",
    "        path_model_and_dependencies = f\"models/sequence_translator_transformer_over_fitted_next.pth\",\n",
    "        root = \"../..\"\n",
    "    )\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e34688-f0a4-4e6f-9a17-2ecd7ff351f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_run = namespace_tools.NameSpaceAggregation(notebook_run)\n",
    "notebook_run.diffuse(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c67373-dd68-4524-b336-c20e333f12de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ploomber_engine.ipython import PloomberClient\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "\n",
    "from translation_machine.models import transformer_mod\n",
    "from translation_machine import sentence_mod\n",
    "\n",
    "\n",
    "# initialize client\n",
    "client = PloomberClient.from_path(Path(\"./training_setup.ipynb\"),cwd=Path(\"./\"))\n",
    "train_setup = client.get_namespace(notebook_run.diffuse())\n",
    "for key,val in train_setup.items():\n",
    "        globals()[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d6670-c590-4d63-af90-4a57d1a75b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# revert to train mode\n",
    "model.train()\n",
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa421e9-c523-4f00-a33a-e5a3e7ee6f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from translation_machine import model_trainer_mod\n",
    "model_trainer = model_trainer_mod.ModelTrainer(model,optimizer,train_data_loader,val_data_loader,baseline_loss,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855a467-c8af-4fe7-b16b-f1c9cd6e08da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_ids,counts = np.unique(np.vstack([el[1] for el in train_data_loader]),return_counts=True)\n",
    "token_id_to_count = dict(zip(token_ids,counts))\n",
    "token_id_to_count = {key:val for (key,val) in token_id_to_count.items() if key !=0}\n",
    "nb_tokens = sum(token_id_to_count.values())\n",
    "token_id_to_freq = {key:val/nb_tokens  for (key,val) in token_id_to_count.items()}\n",
    "token_to_freq = {sentence_mod.FrenchSentence.vocab.itos_[key]:val  for (key,val) in token_id_to_freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67ea22-d913-4f90-a4a7-789cca25934f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict(sorted(token_to_freq.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a2814-4fb5-4799-beb0-4efb9816c4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt,numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "best_epoch = scheduler.last_epoch\n",
    "\n",
    "for epoch in tqdm(range(simple_hp.nb_epochs)):\n",
    "    #import time\n",
    "    #start = time.time()\n",
    "    print(f\"training for epoch {epoch}\")\n",
    "    print(f\"for epoch {epoch} learning rate is {optimizer.param_groups[0]['lr']}\" )\n",
    "    print(\"training_step\")\n",
    "    loss_train,nb_words_per_batch_train,metric_train = model_trainer.train_on_epoch()\n",
    "    print(\"validation_step\")\n",
    "    loss_val,nb_words_per_batch_val,metric_val = model_trainer.validate_on_epoch()\n",
    "\n",
    "    sum_loss_train = torch.tensor(loss_train).sum()\n",
    "    sum_loss_val = torch.tensor(loss_val).sum()\n",
    "    mean_train_loss = sum_loss_train/sum(nb_words_per_batch_train)\n",
    "    mean_val_loss = sum_loss_val/sum(nb_words_per_batch_val)\n",
    "\n",
    "    scheduler.step(mean_val_loss)\n",
    "\n",
    "        \n",
    "    print(f\"for epoch {epoch} mean loss on train {mean_train_loss}\")\n",
    "    print(f\"for epoch {epoch} mean loss on val {mean_val_loss}\")\n",
    "        \n",
    "    losses[\"train\"].append(mean_train_loss)\n",
    "    losses[\"val\"].append(mean_val_loss)\n",
    "    metrics[\"train\"].append(metric_train)\n",
    "    metrics[\"val\"].append(metric_val)\n",
    "    \n",
    "    if (mean_val_loss < best_loss_val_mean):\n",
    "        best_epoch = scheduler.last_epoch\n",
    "        best_loss_val_mean = mean_val_loss\n",
    "\n",
    "        model_training_state = {\"model_params\":model_trainer.model.state_dict(),\n",
    "                               \"model_inputs\":model_inputs,\n",
    "                              \"optimizer\":optimizer.state_dict(),\n",
    "                              \"scheduler\":scheduler.state_dict(),\n",
    "                              }\n",
    "        results = { \"losses\":losses,\n",
    "                   \"metrics\":metrics}\n",
    "        new_back_up = dict()\n",
    "        if \"back_up\" in globals():\n",
    "            new_back_up[\"notebook_runs\"] = back_up[\"notebook_runs\"] + tuple([notebook_run.state_dict()])\n",
    "        else:\n",
    "            new_back_up[\"notebook_runs\"] = tuple([notebook_run.state_dict()])\n",
    "\n",
    "        new_back_up[\"results\"] = results\n",
    "        new_back_up[\"model_training_state\"] = model_training_state\n",
    "        \n",
    "        back_up = new_back_up\n",
    "        torch.save(back_up,paths.path_model_and_dependencies)\n",
    "        print(f\"saving for epoch {epoch}\")\n",
    "        \n",
    "        plt.plot(losses[\"train\"],\"b*\")\n",
    "        plt.plot(losses[\"val\"],\"g*\")\n",
    "        plt.title(\"losses\")\n",
    "        plt.savefig(\"loss_curve\")\n",
    "        #import pdb;pdb.set_trace()\n",
    "    elif epoch - best_epoch > simple_hp.early_stop_thresh  and epoch > simple_hp.warm_up_epochs:\n",
    "        print(\"Early stopped training at epoch %d\" % epoch)\n",
    "        break  # terminate the training loop\n",
    "\n",
    "    del loss_train,nb_words_per_batch_train,metric_train\n",
    "\n",
    "    del loss_val,nb_words_per_batch_val,metric_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571b27e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(results[\"losses\"][\"train\"],\"b*\")\n",
    "plt.plot(results[\"losses\"][\"val\"],\"g*\")\n",
    "plt.title(\"losses\")\n",
    "plt.savefig(f'test.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528ee66-f8f0-4682-be19-098ab5b8854f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
