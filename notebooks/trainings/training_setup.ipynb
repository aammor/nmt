{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9705ce7d",
   "metadata": {},
   "source": [
    "Notebook that show the training setup workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a1e01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### I) define hyper_parameters, datasets, and IO operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f98dcdb4",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# convention : all new inputs parameters for the notebbok through papermil,\n",
    "# that are progressivelt added have a default value, equal to the one tha would be used in\n",
    "# order obtain the same results of scripts\n",
    "\n",
    "# two types of inputs: \n",
    "# inputs that influences the training (e.g hyper-paramteres, dataset set and splitting)\n",
    "# inputs that controls state of training (reset it or load from)\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import argparse\n",
    "@dataclass\n",
    "class NotebookRun:\n",
    "    simple_hyp_params : argparse.Namespace\n",
    "    optimization_control : argparse.Namespace\n",
    "    dataset_control : argparse.Namespace\n",
    "    state_train_control : argparse.Namespace\n",
    "    paths_from_training : argparse.Namespace\n",
    "    \n",
    "    def __hash__(self):\n",
    "        tmp = tuple((\n",
    "            tuple(self.simple_hyp_params.__dict__.items()),\n",
    "            tuple(self.opt_params.__dict__.items()),\n",
    "            tuple(self.dset_truncation.__dict__.items()),\n",
    "            tuple(self.state_train_control.__dict__.items()),\n",
    "            tuple(self.paths_from_training.__dict__.items())\n",
    "        ))\n",
    "        hash_value = hash(tmp)\n",
    "        return hash_value\n",
    "    \n",
    "notebook_run = NotebookRun(simple_hyp_params,opt_params,\n",
    "                           dset_truncation,train_state_control,\n",
    "                           paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bba75435-4269-4466-b7a9-b6997288369f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from translation_machine.models import transformer_mod\n",
    "from translation_machine import sentence_mod\n",
    "\n",
    "model_inputs = {\n",
    "    \"d_model\":simple_hyp_params.d_model,\n",
    "    \"vocab_src\":sentence_mod.EnglishSentence.vocab,\n",
    "    \"vocab_tgt\":sentence_mod.FrenchSentence.vocab,\n",
    "}\n",
    "\n",
    "model = transformer_mod.TransformerForSeq2Seq(**model_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bec56444-d176-4227-8453-6f430f383627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_hyp_params.early_stop_thresh = opt_params.half_period_cycle*simple_hyp_params.early_stop_steps_per_half_clr_cycle,\n",
    "\n",
    "\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=opt_params.base_lr)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,step_size_up= opt_params.half_period_cycle , \n",
    "                                              base_lr=opt_params.base_lr, max_lr=opt_params.max_lr,\n",
    "                                              cycle_momentum=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdcd8e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# II) load the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76840b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 694)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from translation_machine import dataset_mod,sentence_mod\n",
    "\n",
    "import torch,numpy as np\n",
    "\n",
    "language_info = torch.load(paths.path_language_info)\n",
    "\n",
    "vocab_french = language_info[\"french\"][\"vocab\"]\n",
    "vocab_english = language_info[\"english\"][\"vocab\"]\n",
    "\n",
    "\n",
    "len(vocab_french),len(vocab_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c310cfa-9bbf-4cf8-a340-cbbb6f7303a4",
   "metadata": {},
   "source": [
    "# III) Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f71a68-2e7e-4e29-bf84-4d01b0876df9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if dset_truncation.limit_length is None:\n",
    "    dset_truncation.limit_length = language_info[\"limit_length\"]\n",
    "else:\n",
    "    dset_truncation.limit_length = min(language_info[\"limit_length\"],dset_truncation.limit_length)\n",
    "    \n",
    "whole_dataset = dataset_mod.DatasetFromTxt(paths.path_dataset)\n",
    "\n",
    "idxs_whole = np.arange(dset_truncation.limit_length)\n",
    "dataset = torch.utils.data.Subset(whole_dataset,idxs_whole)\n",
    "\n",
    "dataset = list(dataset_mod.SentenceDataSet(dataset,sentence_type_src=sentence_mod.EnglishSentence,sentence_type_dst=sentence_mod.FrenchSentence))\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfc062",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IV) preparation of notebook params for serialization (for the purpose of associating the run to results of training), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93aecd31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_from_file = False\n",
    "if max_length_from_file:\n",
    "    max_length_french = language_info[\"french\"][\"max_sentence_train_val\"]\n",
    "    max_length_english = language_info[\"english\"][\"max_sentence_train_val\"]\n",
    "else:# get max length from current dataset, which is prefered\n",
    "    import itertools\n",
    "    tmp = [(len(el[0]),len(el[1])) for el in dataset]\n",
    "    a,b = zip(*tmp)\n",
    "    max_length_english  = max(a)\n",
    "    max_length_french = max(b)\n",
    "    \n",
    "max_length_english,max_length_french"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c7d7c",
   "metadata": {},
   "source": [
    "### V) dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14419b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15556, 2222, 2222)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remark : the responsability to split the dataset is done outside of this notebook\n",
    "from pathlib import Path\n",
    "\n",
    "if dset_truncation.use_splitting:\n",
    "    path_dataset_splitting = paths.path_dataset_splitting\n",
    "    path_idxs_train = str(Path(path_dataset_splitting).joinpath(\"idxs_train.npy\"))\n",
    "    path_idxs_val = str(Path(path_dataset_splitting).joinpath(\"idxs_val.npy\"))\n",
    "    path_idxs_test = str(Path(path_dataset_splitting).joinpath(\"idxs_test.npy\"))\n",
    "\n",
    "    idxs_train = np.load(path_idxs_train)\n",
    "    idxs_val = np.load(path_idxs_val)\n",
    "    idxs_test = np.load(path_idxs_test)\n",
    "\n",
    "    idxs_train,idxs_val,idxs_test = [[idx for idx in idxs if idx<len(whole_dataset)] for idxs in [idxs_train,idxs_val,idxs_test]]\n",
    "    idxs_train = list(set(idxs_whole).intersection(set(idxs_train)))\n",
    "    idxs_val = list(set(idxs_whole).intersection(set(idxs_val)))\n",
    "    idxs_test = list(set(idxs_whole).intersection(set(idxs_test)))\n",
    "    \n",
    "else:\n",
    "    idxs_train = idxs_whole\n",
    "    idxs_val = idxs_whole\n",
    "    idxs_test = idxs_whole\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset,idxs_train)\n",
    "val_dataset = torch.utils.data.Subset(dataset,idxs_val)\n",
    "test_dataset = torch.utils.data.Subset(dataset,idxs_test)\n",
    "len(train_dataset),len(val_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb11c95",
   "metadata": {},
   "source": [
    "### VI) dataloader construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb015efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from translation_machine import collate_fn_mod\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "collate_fn = collate_fn_mod.get_collate_fn(max_length_english,max_length_french)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset,batch_size=simple_hyp_params.batch_size,\n",
    "                               shuffle=True,collate_fn=collate_fn)\n",
    "val_data_loader = DataLoader(val_dataset,batch_size=simple_hyp_params.batch_size,\n",
    "                             shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea068a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 694)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_french.vocab.itos_),len(vocab_english.vocab.itos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b32119b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import optim,nn\n",
    "\n",
    "from translation_machine import model_trainer\n",
    "\n",
    "baseline_loss = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,step_size_up=half_period_cycle , base_lr=base_lr, max_lr=max_lr)\n",
    "model_trainer = model_trainer.ModelTrainer(model,optimizer,scheduler,train_data_loader,val_data_loader,baseline_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb3966f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model.train()\n",
    "losses = {\"train\":[],\"val\":[]}\n",
    "metrics = {\"train\":[],\"val\":[]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54aec05a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "path_model_and_dependencies = paths.path_model_and_dependencies\n",
    "if train_state_control.load_from_backup and Path(path_model_and_dependencies).exists():\n",
    "    back_up = torch.load(path_model_and_dependencies)\n",
    "    for el1,el2 in zip([model,scheduler,optimizer,scheduler,losses,metrics],\n",
    "                      [\"model_params\",\"scheduler\",\"optimizer\",\"losses\",\"metrics\"]):\n",
    "        if el2 in train_state_control.restore_from_backup:\n",
    "            if el2 == \"losses\":\n",
    "                losses = back_up[el2]\n",
    "            elif el2 == \"metrics\":\n",
    "                metrics = back_up[el2]\n",
    "            else:\n",
    "                el1.load_state_dict(back_up[el2])\n",
    "                \n",
    "    print(\"model loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
