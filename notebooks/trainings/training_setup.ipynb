{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9705ce7d",
   "metadata": {},
   "source": [
    "Notebook that load all python variables necessary for the training, those variables depends on parameters that can be set by the user (in the notebook) or externally using the papermill command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a1e01",
   "metadata": {},
   "source": [
    "### I) define hyper_parameters, datasets, and IO operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98dcdb4",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# convention : all new inputs parameters for the notebbok through papermil,\n",
    "# that are progressivelt added have a default value, equal to the one tha would be used in\n",
    "# order obtain the same results of scripts\n",
    "\n",
    "# two types of inputs: \n",
    "# inputs that influences the training (e.g hyper-paramteres, dataset set and splitting)\n",
    "# inputs that controls state of training (reset it or load from)\n",
    "\n",
    "batch_size= 64\n",
    "d_model = 512\n",
    "early_stopping_activated = False\n",
    "half_period_cycle = 5\n",
    "early_stop_thresh = 3*half_period_cycle\n",
    "nb_epochs = 300\n",
    "\n",
    "\n",
    "path_dataset = \"../../data/french_english_dataset/fra.txt\"\n",
    "limit_length= None\n",
    "use_splitting = True\n",
    "path_language_info = \"../../models/language_info.pth\"\n",
    "path_dataset_splitting = \"../../dataset_splitting\"\n",
    "max_length_from_file = False\n",
    "\n",
    "optimizer_option = \"AdamW\"\n",
    "\n",
    "base_lr = 10**(-6)\n",
    "max_lr = 0.0005\n",
    "momentum = 0.9\n",
    "\n",
    "load_from_backup = True\n",
    "restore_from_backup = tuple([\"model_params\",\"scheduler\",\"optimizer\",\"losses\",\"metrics\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdcd8e1",
   "metadata": {},
   "source": [
    "### II) load the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76840b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 694)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "language_info = torch.load(path_language_info)\n",
    "\n",
    "vocab_french = language_info[\"french\"][\"vocab\"]\n",
    "vocab_english = language_info[\"english\"][\"vocab\"]\n",
    "\n",
    "\n",
    "if limit_length is None:\n",
    "    limit_length = language_info[\"limit_length\"]\n",
    "else:\n",
    "    limit_length = min(language_info[\"limit_length\"],limit_length)\n",
    "len(vocab_french),len(vocab_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfc062",
   "metadata": {},
   "source": [
    "### III) preparation of notebook params for serialization (for the purpose of associating the run to results of training), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95dc58c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# by simple hyper\n",
    "_local_variable =locals()\n",
    "simple_hyper_parameters = {key:_local_variable[key] for key in [\"batch_size\",\"d_model\",\n",
    "                                                                \"early_stopping_activated\",\"half_period_cycle\",\n",
    "                                                                \"early_stop_thresh\",\"nb_epochs\"]}\n",
    "\n",
    "dataset_control = {key:_local_variable[key] for key in [\"path_dataset\",\"path_language_info\",\n",
    "                                                                \"limit_length\",\"use_splitting\"]}\n",
    "\n",
    "optimization_control = {key:_local_variable[key] for key in [\"optimizer_option\",\"base_lr\",\n",
    "                                                                \"max_lr\",\"momentum\"]}\n",
    "\n",
    "state_train_control = {key:_local_variable[key] for key in [\"load_from_backup\",\"restore_from_backup\"]}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NotebookRun:\n",
    "    simple_hyper_parameters : dict\n",
    "    optimization_control : dict\n",
    "    dataset_control : dict\n",
    "    state_train_control : dict\n",
    "    \n",
    "    def __hash__(self):\n",
    "        tmp = tuple((\n",
    "            tuple(self.simple_hyper_parameters.items()),\n",
    "            tuple(self.optimization_control.items()),\n",
    "            tuple(self.dataset_control.items()),\n",
    "            tuple(self.state_train_control.items())         \n",
    "        ))\n",
    "        hash_value = hash(tmp)\n",
    "        return hash_value\n",
    "    \n",
    "notebook_run = NotebookRun(simple_hyper_parameters,optimization_control,dataset_control,state_train_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa29b04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from translation_machine import dataset_mod,sentence_mod\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "whole_dataset = dataset_mod.DatasetFromTxt(path_dataset)\n",
    "if limit_length is not None:\n",
    "    idxs_whole = np.arange(limit_length)\n",
    "    dataset = torch.utils.data.Subset(whole_dataset,idxs_whole)\n",
    "else:\n",
    "    idxs_whole = np.arange(len(whole_dataset))\n",
    "    dataset = whole_dataset\n",
    "    \n",
    "    \n",
    "dataset = list(dataset_mod.SentenceDataSet(dataset,sentence_type_src=sentence_mod.EnglishSentence,sentence_type_dst=sentence_mod.FrenchSentence))\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93aecd31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_from_file = False\n",
    "if max_length_from_file:\n",
    "    max_length_french = language_info[\"french\"][\"max_sentence_train_val\"]\n",
    "    max_length_english = language_info[\"english\"][\"max_sentence_train_val\"]\n",
    "else:# get max length from current dataset, which is prefered\n",
    "    import itertools\n",
    "    tmp = [(len(el[0]),len(el[1])) for el in dataset]\n",
    "    a,b = zip(*tmp)\n",
    "    max_length_english  = max(a)\n",
    "    max_length_french = max(b)\n",
    "    \n",
    "max_length_english,max_length_french"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262c7d7c",
   "metadata": {},
   "source": [
    "### IV) Storage of notebook params (for serialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14419b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15556, 2222, 2222)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remark : the responsability to split the dataset is done outside of this notebook\n",
    "from pathlib import Path\n",
    "\n",
    "if use_splitting:\n",
    "    path_idxs_train = str(Path(path_dataset_splitting).joinpath(\"idxs_train.npy\"))\n",
    "    path_idxs_val = str(Path(path_dataset_splitting).joinpath(\"idxs_val.npy\"))\n",
    "    path_idxs_test = str(Path(path_dataset_splitting).joinpath(\"idxs_test.npy\"))\n",
    "\n",
    "    idxs_train = np.load(path_idxs_train)\n",
    "    idxs_val = np.load(path_idxs_val)\n",
    "    idxs_test = np.load(path_idxs_test)\n",
    "\n",
    "    idxs_train,idxs_val,idxs_test = [[idx for idx in idxs if idx<len(whole_dataset)] for idxs in [idxs_train,idxs_val,idxs_test]]\n",
    "    idxs_train = list(set(idxs_whole).intersection(set(idxs_train)))\n",
    "    idxs_val = list(set(idxs_whole).intersection(set(idxs_val)))\n",
    "    idxs_test = list(set(idxs_whole).intersection(set(idxs_test)))\n",
    "    \n",
    "else:\n",
    "    idxs_train = idxs_whole\n",
    "    idxs_val = idxs_whole\n",
    "    idxs_test = idxs_whole\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset,idxs_train)\n",
    "val_dataset = torch.utils.data.Subset(dataset,idxs_val)\n",
    "test_dataset = torch.utils.data.Subset(dataset,idxs_test)\n",
    "len(train_dataset),len(val_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb11c95",
   "metadata": {},
   "source": [
    "### 2) dataloader construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb015efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from translation_machine import collate_fn_mod\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "collate_fn = collate_fn_mod.get_collate_fn(max_length_english,max_length_french)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,collate_fn=collate_fn)\n",
    "val_data_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea068a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 694)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_french.vocab.itos_),len(vocab_english.vocab.itos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a907485b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from translation_machine.models import transformer_mod\n",
    "\n",
    "model_inputs = {\n",
    "    \"d_model\":d_model,\n",
    "    \"vocab_src\":sentence_mod.EnglishSentence.vocab,\n",
    "    \"vocab_tgt\":sentence_mod.FrenchSentence.vocab,\n",
    "}\n",
    "\n",
    "model = transformer_mod.TransformerForSeq2Seq(**model_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b32119b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import optim,nn\n",
    "\n",
    "from translation_machine import model_trainer\n",
    "\n",
    "baseline_loss = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "\n",
    "if optimizer_option == \"AdamW\":\n",
    "    optimizer = torch.optim.NAdam(model.parameters(), lr=base_lr)\n",
    "elif optimizer_option == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=momentum)\n",
    "else:\n",
    "    raise ValueError\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,step_size_up=half_period_cycle , base_lr=base_lr, max_lr=max_lr,cycle_momentum=False)\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,step_size_up=half_period_cycle , base_lr=base_lr, max_lr=max_lr)\n",
    "model_trainer = model_trainer.ModelTrainer(model,optimizer,scheduler,train_data_loader,val_data_loader,baseline_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb3966f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model.train()\n",
    "losses = {\"train\":[],\"val\":[]}\n",
    "metrics = {\"train\":[],\"val\":[]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54aec05a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "path_model_and_dependencies = \"../../models/sequence_translator_transformer.pth\"\n",
    "\n",
    "if load_from_backup and Path(path_model_and_dependencies).exists():\n",
    "    back_up = torch.load(path_model_and_dependencies)\n",
    "    for el1,el2 in zip([model,scheduler,optimizer,scheduler,losses,metrics],\n",
    "                      [\"model_params\",\"scheduler\",\"optimizer\",\"losses\",\"metrics\"]):\n",
    "        if el2 in restore_from_backup:\n",
    "            if el2 == \"losses\":\n",
    "                losses = back_up[el2]\n",
    "            elif el2 == \"metrics\":\n",
    "                metrics = back_up[el2]\n",
    "            else:\n",
    "                el1.load_state_dict(back_up[el2])\n",
    "                \n",
    "    print(\"model loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
